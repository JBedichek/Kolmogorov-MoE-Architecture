# MoE Transformer Training Configuration - 500M Model (Low Memory)

# Model configuration
model:
  # Architecture (~1.2B parameters with 32 layers)
  vocab_size: 50257
  d_model: 768
  n_layers: 32          # Increased to 32
  n_heads: 12
  n_kv_heads: 3         # GQA 4:1 ratio
  head_dim: 64
  d_ff: 2048
  max_seq_len: 4096

  # MoE configuration
  n_experts: 8
  moe_top_k: 2
  moe_capacity_factor: 1.25
  d_ff_expert: 1024
  moe_load_balance_loss_weight: 0.01
  moe_router_z_loss_weight: 0.001
  moe_layers: [2, 3, 6, 7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31]  # 16 MoE layers

  # MoD configuration
  mod_enabled: false
  mod_capacity_factor: 0.75
  mod_router_hidden_dim: 128
  mod_load_balance_loss_weight: 0.01

  # Mamba configuration
  mamba_enabled: false
  ssm_state_dim: 16
  ssm_expansion: 2
  mamba_layers: []

  # Multi-token prediction
  n_pred_tokens: 1      # Reduced from 4 to save memory
  aux_loss_weights: [1.0]

  # Optimizations
  use_flash_attention: true
  rope_theta: 10000.0
  norm_type: "rmsnorm"
  ffn_activation: "swiglu"
  dropout: 0.1
  attention_dropout: 0.1
  residual_dropout: 0.1

# Training configuration
training:
  total_tokens: 100_000_000_000  # 100B tokens
  batch_size: 1
  gradient_accumulation_steps: 8  # Increased to compensate for smaller batch

  max_lr: 0.001  # Muon optimizer
  min_lr_ratio: 0.1
  warmup_steps: 100

  gradient_clip_norm: 1.0
  weight_decay: 0.1

  checkpoint_dir: "./checkpoints_hf_500m"
  log_interval: 10
  eval_interval: 1000
  save_interval: 1000

# Optimizer configuration
optimizer:
  type: "muon"  # Hybrid Muon + AdamW
  momentum: 0.95
  weight_decay: 0.01
  nesterov: true
  backend: "newton"
  newton_iters: 5

# Data configuration
data:
  tokenizer_name: "gpt2"
  use_memory_mapped: false
  data_path: null
  num_workers: 2

# HuggingFace Trainer specific settings
huggingface:
  # Mixed precision
  bf16: true

  # Logging
  logging_dir: "./logs_hf_500m"
  use_wandb: false

  # Checkpointing
  save_total_limit: 3

  # Reproducibility
  seed: 42

# Weights & Biases (optional)
wandb:
  project: "moe-architecture"
  run_name: "moe-500m-hf"
  entity: null
