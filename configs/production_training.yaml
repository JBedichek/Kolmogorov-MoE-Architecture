# Production Training Configuration for MoE with FineWeb
# Target: ~7.6B total params with MAXIMUM SPARSITY
# Only 8.1% active params (0.62B active out of 7.64B total)
# 64 experts with top_k=2 for proper router training

# Model configuration
model:
  vocab_size: 32000  # Llama 2 tokenizer vocab size (4x smaller, minimal compression loss for English)
  d_model: 2048
  n_layers: 14
  n_heads: 8
  n_kv_heads: 4  # GQA 4:1 ratio
  head_dim: 256
  d_ff: 5504  # Only used for non-MoE layers (if any)
  max_seq_len: 2048

  # MoE configuration - 16 experts with top_k=2
  # Total: 2.83B params, Active: 0.47B params (83.5% sparsity)
  n_experts: 16
  moe_top_k: 4
  moe_capacity_factor: 1.25
  d_ff_expert: 2048

  # Routing strategy:
  # - "token_choice": Tokens select top-k experts (standard, can collapse without aux loss)
  # - "expert_choice": Experts select top-k tokens (perfect balance, collapse-resistant)
  # Expert-choice is recommended if experiencing router collapse
  moe_routing: "expert_choice"
  # Balanced routing (for token_choice only): enforces capacity constraints per expert
  # Each expert can only handle (n_tokens * top_k / n_experts) tokens per batch
  # This prevents collapse while maintaining token-choice semantics
  moe_balanced_routing: false

  # Auxiliary loss weights (only used with token_choice routing)
  # With expert_choice routing, load balance is perfect by construction
  moe_load_balance_loss_weight: 0.1
  moe_router_z_loss_weight: 0.01

  moe_layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
  # MoE implementation: "batched" (padded bmm), "sparse" (no padding), "expert_parallel" (multi-GPU)
  # Note: expert_parallel conflicts with FSDP - use sparse or batched instead
  moe_implementation: "batched"

  # MoD configuration - Token skipping for compute efficiency
  # IMPORTANT: Start with higher capacity, reduce gradually as model learns
  # 0.25 was too aggressive - caused loss plateau at ~8
  mod_enabled: false
  mod_capacity_factor: 0.75  # 75% of tokens processed (25% skipped)
  mod_router_hidden_dim: 64
  mod_load_balance_loss_weight: 0.01

  # Mamba configuration - DISABLED (dense, adds active params)
  mamba_enabled: false
  mamba_layers: []

  # Multi-token prediction
  n_pred_tokens: 1
  aux_loss_weights: [1.0]

  # Optimizations
  use_flash_attention: true
  rope_theta: 10000.0
  norm_type: "rmsnorm"
  ffn_activation: "swiglu"
  dropout: 0.0  # No dropout for faster initial training
  attention_dropout: 0.0
  residual_dropout: 0.0

# Training configuration
training:
  total_tokens: 10_000_000_000  # 10B tokens
  batch_size: 4
  # NOTE: Higher grad_accum delays optimizer.step() which causes memory spike
  # when all optimizer state is allocated at once. Keep lower for memory.
  gradient_accumulation_steps: 2  # Effective batch = 8 (was 32, caused OOM spike)

  # Learning rate - reduced from 0.003 which was too high
  max_lr: 0.0006  # Standard LLM learning rate (6e-4)
  min_lr_ratio: 0.1
  warmup_steps: 1000  # Longer warmup for stability

  # Optimization
  gradient_clip_norm: 1.0
  weight_decay: 0.01

  # Checkpointing
  checkpoint_dir: "./checkpoints_production"
  save_interval: 1000

  # Logging
  log_interval: 10
  eval_interval: 500
  resume_from: null

# Optimizer configuration
# NOTE: Muon's Newton-Schulz orthogonalization creates large temporary tensors
# during optimizer.step(). For memory-constrained training, use adamw instead.
optimizer:
  type: "adamw"  # Options: "muon", "adamw" - adamw uses less peak memory
  # AdamW settings
  adamw_lr: 0.00005  # Standard LLM learning rate for AdamW
  adamw_betas: [0.9, 0.999]
  # Muon settings (typically uses higher LR than AdamW)
  muon_lr: 0.01  # Muon can use 10-100x higher LR
  momentum: 0.95
  nesterov: true
  # Shared
  weight_decay: 0.01

# Data configuration
data:
  dataset_name: "HuggingFaceFW/fineweb"
  dataset_config: "sample-10BT"
  tokenizer_name: "meta-llama/Llama-2-7b-hf"

  # Number of examples to load (null = all available)
  max_examples: null  # Set to number like 100000 to limit dataset size

  # Streaming vs pre-loading
  use_streaming: false  # False = load into memory (faster but needs RAM)

  # Tokenization strategy
  tokenize_on_fly: true  # True = tokenize during training (saves memory, slightly slower)

  # Workers
  num_workers: 8

  # IMPORTANT: Set to false to enable Flash Attention (17x faster)
  # True creates document-aware masks which disable Flash Attention
  use_document_attention_mask: false

# Mixed precision
mixed_precision:
  bf16: true
  fp8: false

# Weights & Biases logging
wandb:
  enabled: false
  project: "moe-8b-sparse"
  run_name: "7.6b-64experts-620m-active"
  entity: null

# Evaluation
evaluation:
  enabled: false
  eval_steps: 100
