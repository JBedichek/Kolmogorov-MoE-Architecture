# Quick test configuration to verify production script works
# This runs for only 500K tokens (~122 steps) to verify everything works

model:
  vocab_size: 50257
  d_model: 512
  n_layers: 6
  n_heads: 8
  n_kv_heads: 2
  head_dim: 64
  d_ff: 2048
  max_seq_len: 512
  n_experts: 8
  moe_top_k: 2
  moe_capacity_factor: 1.25
  d_ff_expert: 1024
  moe_load_balance_loss_weight: 0.01
  moe_router_z_loss_weight: 0.001
  moe_layers: [1, 2, 4, 5]
  mod_enabled: true
  mod_capacity_factor: 0.75
  mod_router_hidden_dim: 128
  mod_load_balance_loss_weight: 0.01
  mamba_enabled: false
  mamba_layers: []
  n_pred_tokens: 1
  aux_loss_weights: [1.0]
  use_flash_attention: false
  rope_theta: 10000.0
  norm_type: "rmsnorm"
  ffn_activation: "swiglu"
  dropout: 0.0
  attention_dropout: 0.0
  residual_dropout: 0.0

training:
  total_tokens: 500_000  # Only 500K tokens for quick test
  batch_size: 1
  gradient_accumulation_steps: 8
  max_lr: 0.001
  min_lr_ratio: 0.1
  warmup_steps: 10
  gradient_clip_norm: 1.0
  weight_decay: 0.01
  checkpoint_dir: "./test_checkpoints"
  save_interval: 50
  log_interval: 1
  eval_interval: 50
  resume_from: null

optimizer:
  type: "muon"
  momentum: 0.95
  weight_decay: 0.01
  nesterov: true

data:
  dataset_name: "HuggingFaceFW/fineweb"
  dataset_config: "sample-10BT"
  tokenizer_name: "gpt2"
  max_examples: 2000  # Only 2K examples for speed
  use_streaming: false
  num_workers: 2

mixed_precision:
  bf16: true

wandb:
  enabled: false
  project: "moe-test"
  run_name: "test-run"
  entity: null

evaluation:
  enabled: true
  eval_steps: 50
